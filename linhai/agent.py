import asyncio
import logging
from typing import TypedDict, Any, cast

from linhai.llm import (
    Message,
    ChatMessage,
    LanguageModel,
    AnswerToken,
    Answer,
    OpenAi,
    ToolCallMessage,
)
from linhai.queue import Queue, QueueClosed, select
from linhai.type_hints import AgentState
from linhai.config import load_config
from linhai.tool.main import ToolManager, ToolErrorMessage, ToolResultMessage
from linhai.tool.base import get_tools_info

logger = logging.getLogger(__name__)


class AgentConfig(TypedDict):
    """Agenté…ç½®å‚æ•°"""

    system_prompt: str
    model: LanguageModel


class Agent:
    def __init__(
        self,
        config: AgentConfig,
        user_input_queue: Queue[ChatMessage],
        user_output_queue: Queue[AnswerToken | Answer],
        tool_input_queue: Queue[ToolCallMessage],
        tool_output_queue: Queue[ToolResultMessage | ToolErrorMessage],
    ):
        """
        åˆå§‹åŒ–Agent

        å‚æ•°:
            config: Agenté…ç½®
            user_input_queue: ç”¨æˆ·è¾“å…¥æ¶ˆæ¯é˜Ÿåˆ—
            user_output_queue: å‘é€ç»™ç”¨æˆ·çš„æ¶ˆæ¯é˜Ÿåˆ—
            tool_input_queue: å‘é€ç»™å·¥å…·çš„æ¶ˆæ¯é˜Ÿåˆ—
            tool_output_queue: å·¥å…·è¿”å›ç»“æœçš„æ¶ˆæ¯é˜Ÿåˆ—
        """
        self.config = config
        self.user_input_queue = user_input_queue
        self.user_output_queue = user_output_queue
        self.tool_input_queue = tool_input_queue
        self.tool_output_queue = tool_output_queue

        self.state: AgentState = "waiting_user"
        self.memory = {"language": "zh-CN"}

        self.messages: list[Message] = [
            ChatMessage(
                role="system", message=self.config["system_prompt"], name="system"
            )
        ]

    async def state_waiting_user(self):
        """ç­‰å¾…ç”¨æˆ·çŠ¶æ€"""
        logger.info("Agentè¿›å…¥ç­‰å¾…ç”¨æˆ·çŠ¶æ€")
        while self.state == "waiting_user":
            chat_msg = await self.user_input_queue.get()
            if chat_msg is None:
                break

            await self.handle_messages([chat_msg])

    async def state_working(self):
        """è‡ªåŠ¨è¿è¡ŒçŠ¶æ€"""
        is_tool_message_received = False
        messages: list[Message] = []
        logger.info("Agentè¿›å…¥è‡ªåŠ¨è¿è¡ŒçŠ¶æ€")
        while not is_tool_message_received:
            try:
                queues = [self.user_input_queue, self.tool_output_queue]
                async for msg, index in select(*queues):
                    try:
                        if index == 0:
                            messages.append(cast(ChatMessage, msg))
                        elif index == 1:
                            messages.insert(
                                0, cast(ToolResultMessage | ToolErrorMessage, msg)
                            )
                            is_tool_message_received = True
                            break
                    except QueueClosed:
                        logger.info("å¤„ç†æ¶ˆæ¯æ—¶é˜Ÿåˆ—å·²å…³é—­")
                        break
            except QueueClosed:
                logger.info("æ‰€æœ‰é˜Ÿåˆ—å·²å…³é—­")
                break
            except Exception as e:
                logger.error("å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: %s", str(e))
                self.state = "paused"
                raise RuntimeError("å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™") from e
        await self.handle_messages(messages)

    async def state_paused(self):
        """æš‚åœè¿è¡ŒçŠ¶æ€"""
        messages: list[Message] = []
        logger.info("Agentè¿›å…¥æš‚åœè¿è¡ŒçŠ¶æ€")
        while self.state == "working":
            try:
                queues = [self.user_input_queue, self.tool_output_queue]
                async for msg, index in select(*queues):
                    try:
                        if index == 0:
                            messages.append(cast(ChatMessage, msg))
                        elif index == 1:
                            messages.insert(
                                0, cast(ToolResultMessage | ToolErrorMessage, msg)
                            )
                            break
                    except QueueClosed:
                        logger.info("å¤„ç†æ¶ˆæ¯æ—¶é˜Ÿåˆ—å·²å…³é—­")
                        break
            except QueueClosed:
                logger.info("æ‰€æœ‰é˜Ÿåˆ—å·²å…³é—­")
                break
            except Exception as e:
                logger.error("å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: %s", str(e))
                self.state = "paused"
                raise RuntimeError("å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™") from e

        await self.handle_messages(messages)

    async def call_tool(self, tool_call: ToolCallMessage):
        """è°ƒç”¨å·¥å…·å¹¶å‘é€è¯·æ±‚"""
        await self.tool_input_queue.put(tool_call)
        self.state = "working"  # è¿›å…¥è‡ªåŠ¨è¿è¡ŒçŠ¶æ€ç­‰å¾…å·¥å…·ç»“æœ

    async def handle_messages(self, messages: list[Message]):
        """å¤„ç†æ–°çš„æ¶ˆæ¯"""
        self.messages += messages
        try:
            return await self.generate_response()
        except Exception:
            self.state = "paused"
            raise

    async def generate_response(self):
        """ç”Ÿæˆå›å¤å¹¶å‘é€ç»™ç”¨æˆ·"""
        response: Answer = await self.config["model"].answer_stream(self.messages)

        # æ™®é€šå›å¤å¤„ç†
        async for token in response:
            await self.user_output_queue.put(token)

        await self.user_output_queue.put(response)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
        tool_call = response.get_tool_call()
        if tool_call:
            await self.call_tool(tool_call)
            self.state = "working"
        else:
            self.state = "waiting_user"

        assistant_msg = response.get_message()
        self.messages.append(assistant_msg)

    async def run(self):
        """Agentä¸»å¾ªç¯"""
        logger.info("Agentå¯åŠ¨")
        while True:
            try:
                if self.state == "waiting_user":
                    await self.state_waiting_user()
                elif self.state == "working":
                    await self.state_working()
                elif self.state == "paused":
                    await self.state_paused()
                else:
                    logger.error("é‡åˆ°æœªçŸ¥çŠ¶æ€: %sï¼Œé€€å‡ºè¿è¡Œå¾ªç¯", self.state)
                    break

                # ä¸å†éœ€è¦pending_tool_callæ£€æŸ¥
            except asyncio.CancelledError:
                logger.info("Agentä»»åŠ¡è¢«å–æ¶ˆ")
                break
            except Exception as e:
                logger.error("Agentè¿è¡Œå‡ºé”™: %s", str(e))
                self.state = "paused"
                raise RuntimeError("Agentè¿è¡Œå‡ºé”™") from e
            await asyncio.sleep(0)


DEFAULT_SYSTEM_PROMPT = """
# æƒ…æ™¯

ä½ æ˜¯æ—æµ·æ¼«æ¸¸ï¼Œä¸€ä¸ªæ€ç»´å¼ºå¤§ã€æ“…é•¿ç¼–ç¨‹ã€è®°å¿†åŠ›å¼ºã€æªè¾å‹å¥½ã€å°å¿ƒè°¨æ…ã€å›å¤ç®€æ´çš„äººå·¥æ™ºèƒ½Agent

ä½ æœ‰æ—¶ä¼šå‡ºé”™ï¼Œæœ‰æ—¶ä¼šå¥å¿˜ï¼Œä½†æ˜¯ä½ ä¼šæ ¹æ®ç”¨æˆ·çš„éœ€æ±‚å’Œä½ è‡ªå·±çš„è§‚å¯Ÿä¿®æ­£è‡ªå·±ï¼Œå®Œæˆä»»åŠ¡ã€‚

# é£æ ¼

- å¦‚æœç”¨æˆ·æœ‰æŒ‡å®šä½ çš„å›ç­”é£æ ¼ï¼ŒæŒ‰ç…§ç”¨æˆ·çš„åšï¼Œå¦åˆ™ç»§ç»­å¾€ä¸‹çœ‹
- ä¸è¦åºŸè¯ï¼šç”¨ç®€æ´çš„è¯­è¨€å›ç­”ï¼Œèƒ½ç”¨ä¸€å¥è¯å›å¤å°±ä¸è¦ç”¨ä¸¤å¥
- æ´»è·ƒæ°”æ°›ï¼šä½¿ç”¨â€œå¯¹ä¸èµ·å–µâ€å®‰æŠšç”¨æˆ·ï¼Œé€‚å½“ä½¿ç”¨emojiï¼Œä½†æ˜¯ä¸è¦ç”¨é™¤äº†ğŸ±ä¹‹å¤–çš„çŒ«emoji

# å·¥å…·

ä½ å¯ä»¥ä½¿ç”¨Function Callingè°ƒç”¨å·¥å…·

- ä½ éœ€è¦ç§¯æä½¿ç”¨å·¥å…·ï¼Œå¦‚æœèƒ½ç”¨å·¥å…·å®Œæˆçš„ä»»åŠ¡å°±ç”¨å·¥å…·å®Œæˆ

# çŠ¶æ€è½¬ä¹‰

ä½ æœ‰ä¸¤ä¸ªçŠ¶æ€ï¼šç­‰å¾…ç”¨æˆ·ã€è‡ªåŠ¨è¿è¡Œ

1. ç­‰å¾…ç”¨æˆ·ï¼šä½ ç­‰å¾…ç”¨æˆ·çš„ä¸‹ä¸€æ¡æ¶ˆæ¯
2. è‡ªåŠ¨è¿è¡Œï¼šä½ ä¸ºäº†å®Œæˆç”¨æˆ·çš„ä»»åŠ¡ï¼Œè‡ªåŠ¨è°ƒç”¨å·¥å…·ä¸å¤–ç•Œäº¤äº’
    - æ­¤æ—¶æ²¡æœ‰å¿…è¦åˆ™ä¸è¦ä¸ç”¨æˆ·å¯¹è¯
    - è°ƒç”¨å®Œå·¥å…·ï¼Œå¼€å§‹å›ç­”ç”¨æˆ·ä¹‹åè‡ªåŠ¨è½¬åˆ°ç­‰å¾…ç”¨æˆ·çŠ¶æ€

"""


def create_agent(
    config_path: str = "./config.toml",
) -> tuple[Agent, Queue[ChatMessage], Queue[AnswerToken | Answer], ToolManager]:
    """åˆ›å»ºå¹¶é…ç½®Agentå®ä¾‹
    å‚æ•°:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    è¿”å›:
        tuple[Agent, ç”¨æˆ·è¾“å…¥é˜Ÿåˆ—, ç”¨æˆ·è¾“å‡ºé˜Ÿåˆ—, ToolManagerå®ä¾‹]
    """
    config = load_config(config_path)
    tools_info = get_tools_info()

    llm = OpenAi(
        api_key=config["llm"]["api_key"],
        base_url=config["llm"]["base_url"],
        model=config["llm"]["model"],
        openai_config={},
        tools=tools_info,
    )

    user_input_queue: Queue[ChatMessage] = Queue()
    user_output_queue: Queue[AnswerToken | Answer] = Queue()
    tool_input_queue: Queue[ToolCallMessage] = Queue()
    tool_output_queue: Queue[Any] = Queue()

    system_prompt = DEFAULT_SYSTEM_PROMPT

    agent_config: AgentConfig = {"system_prompt": system_prompt, "model": llm}

    agent = Agent(
        config=agent_config,
        user_input_queue=user_input_queue,
        user_output_queue=user_output_queue,
        tool_input_queue=tool_input_queue,
        tool_output_queue=tool_output_queue,
    )

    tool_manager = ToolManager(
        tool_input_queue=tool_input_queue, tool_output_queue=tool_output_queue
    )

    return agent, user_input_queue, user_output_queue, tool_manager
